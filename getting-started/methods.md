---
icon: arrow-progress
---

# Описание методов предобработки, кластеризации и классификации

### Описание методов

#### 1) Предобработка

* `normalize_anglicisms` — унификация терминов.
* `normalize_ru` (pymorphy2) — лемматизация, удаление предлогов/союзов/частиц, стоп-слов и чисел; латиница и бренды сохраняются.
* `prepare_text(title, text)` — сборка и очистка для витрины тем.
* Два представления текста:
  * `docs_ctx` — сырые (`title + text`);
  * `docs_clean` — очищенные (леммы) для именования тем.
* Длинные отзывы: чанкинг до `max_chars` (по умолчанию 1000) с последующим пуллингом.

#### 2) Эмбеддинги и пуллинг

* Модель эмбеддингов: `SentenceTransformer` (по умолчанию `BAAI/bge-m3`).
* Чанки кодируются по отдельности; агрегирование (pooling):
  * `max` (по умолчанию),
  * `mean`,
  * `attn_len` (взвешивание по длине).
* Итоговый вектор L2-нормализуется.

#### 3) Тематическая кластеризация (BERTopic)

* Векторизатор для названий тем:
  * `CountVectorizer(ngram_range=(1,3), min_df=3, max_df=0.4, token_pattern=...)`
  * доменные стоп-слова (банк, карта, приложение и пр.).
* Кластеризация эмбеддингов:
  * по умолчанию KMeans (в BERTopic 0.17.x передаётся как `hdbscan_model=KMeans(...)`);
  * альтернатива — HDBSCAN; опционально `reduce_outliers` для перераспределения `-1`.
* Улучшение названий тем:
  * `update_topics(..., representation_model=MaximalMarginalRelevance(diversity=0.6))`.
* Выход:
  * `topic_info` — перечень тем,
  * `doc_info` — тема по документу.

#### 4) Классификация (мультилейбл, zero-shot на эмбеддингах)

* Словарь бизнес-классов (`src/final_classes.py`) в формате multi-prototype: по несколько фраз-прототипов на класс.
* Для каждого класса считаются эмбеддинги прототипов и усредняются ⇒ вектор класса.
* Для документа берётся его эмбеддинг; считаются косинусы к векторам классов.
* Отбор меток:
  * top-k классов на документ (значение `k` настраивается), **или**
  * по порогу косинуса.
* Для удобства ранжирования используются квазивероятности `softmax_cos` по косинусам (температура `τ = 0.07`).

#### 5) Маппинг тем → бизнес-классы (zero-shot)

* Кодируются top-слова темы ⇒ вектор темы.
* Считаются косинусы «тема ↔ класс»; берётся top-k классов для темы.
* Отчёт с маппингом сохраняется в `bertopic_outputs.xlsx` (лист `topics_top3_classes`).

#### 6) Артефакты

* `artifacts/latest/bertopic_model/` — модель BERTopic.
* `artifacts/latest/embedder/` — сохранённый SentenceTransformer.
* `artifacts/latest/class_vecs.npy`, `class_names.json` — матрица векторов классов и имена.
* `artifacts/latest/bertopic_outputs.xlsx` — отчёт (темы, маппинги, распределения, «сомнительные» темы).
